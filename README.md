# Asahi Whisper Daemon

Voice dictation system using whisper.cpp for Asahi Linux on Apple Silicon.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Platform](https://img.shields.io/badge/platform-Asahi%20Linux-blue)](https://asahilinux.org/)

## Features

- ðŸŽ¤ **Three dictation modes**:
  - **CLI Mode (â—)** - Press-to-record (SUPER+D)
  - **Server Mode (â—†)** - Model stays in memory for faster transcription
  - **Stream Mode (â–¶)** - Live transcription with Voice Activity Detection
- ðŸ“Š **Waybar integration** - Visual mode and status indicators
- ðŸ”Š **Audio feedback** - Snare/hihat sounds for start/stop
- âš¡ **Fast transcription** - 11.5x faster than real-time (base.en)
- ðŸŽ›ï¸ **Interactive model switcher** - Right-click menu to change models
- ðŸ’¾ **Flexible memory usage** - 300 MB (CLI) to 1.7 GB (Server with large models)
- ðŸš€ **Auto-start** - Systemd service starts on boot
- ðŸŽ¯ **Optimized for M1/M2/M3/M4** - ARM NEON, FP16, DOTPROD instructions

## Demo

**Waybar Indicator Modes:**
- `ã€°` - CLI mode ready (loads model each time)
- `â—†` - Server mode ready (model in memory)
- `ã€° streaming` - Stream mode active (live transcription)

**States:**
- `â— dictation` - Currently recording (CLI mode)
- `â—† dictation` - Currently recording (Server mode)
- `dictation` - Processing transcription
- `ã€° streaming` - Streaming mode active (VAD listening)

## Requirements

### Hardware
- Apple Silicon Mac (M1, M2, M3, M4, etc.)
- Running [Asahi Linux](https://asahilinux.org/)

### Software
- **Sway** - Wayland compositor
- **Waybar** - Status bar
- **wtype** - Wayland text input tool
- **uv** - Python package manager
- **Build tools** - git, cmake, gcc/g++

### Installing Prerequisites

**Fedora/RHEL:**
```bash
sudo dnf install sway waybar wtype git cmake gcc-c++
```

**Installing uv:**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

## Installation

### Quick Install

```bash
git clone https://github.com/calebbornman/asahi-whisper-daemon.git
cd asahi-whisper-daemon
./install.sh
```

The install script will:
1. Check for Asahi Linux (warns if not detected)
2. Verify all prerequisites
3. Clone and build whisper.cpp
4. Download the base.en model
5. Create Python virtual environment
6. Install dependencies
7. Configure systemd service
8. Update Sway and Waybar configs
9. Start the daemon

### Installation Options

```bash
./install.sh [OPTIONS]

Options:
  --dry-run          Preview installation without making changes
  --model NAME       Whisper model to download (default: base.en)
  --whisper-dir DIR  Custom whisper.cpp location (default: ~/projects/whisper.cpp)
  --help             Show help message
```

**Examples:**

```bash
# Preview what will be installed
./install.sh --dry-run

# Install with a more accurate model
./install.sh --model small.en

# Custom whisper.cpp location
./install.sh --whisper-dir ~/.local/share/whisper.cpp
```

## Usage

### Three Dictation Modes

The daemon supports three different modes for different use cases:

| Mode | Icon | Keybind | Best For |
|------|------|---------|----------|
| **Stream Mode** | â–¶ | SUPER+D (toggle stream) | Live transcription as you speak |
| **CLI Mode** | â— | SUPER+Shift+D (toggle recording) | Quick dictation, lower memory usage |
| **Server Mode** | â—† | Via menu only | Faster transcription with larger models |

### 1. Live Streaming Mode (â–¶) - Default

Real-time transcription as you speak using Voice Activity Detection (VAD):

1. **Press SUPER+D** to start streaming
   - Waybar shows: `â–¶` 
   - Hear: snare sound (starts immediately - no delay!)
2. **Start speaking** - just talk naturally
3. **Pause briefly** - VAD detects silence and transcribes
4. **Text appears** automatically after each pause
5. **Continue speaking** or pause for 10+ seconds to reset context
6. **Press SUPER+D** again to stop streaming
   - Hear: hihat sound

**Streaming Mode Features:**
- âœ… **Instant startup** - No delay before you can speak
- âœ… **Smart deduplication** - Prevents repeated text when buffer shifts
- âœ… **Fuzzy matching** - Handles punctuation changes ("Excellent." vs "Excellent,")
- âœ… **Auto-reset** - Context clears after 10s silence (prevents slowdown)
- âœ… **Handles apostrophes** - "it's", "I'm", "you're" all work perfectly
- âœ… **Long sessions** - Tested for hours of continuous use
- âœ… **Always-on ready** - Leave it running, only uses CPU when speaking
- ðŸŽ¯ **30-second buffer** - Rolling window with Voice Activity Detection
- ðŸ“Š **~300 MB memory** - Constant, doesn't grow over time

### 2. Basic Dictation (CLI Mode) - SUPER+Shift+D

Press-to-record mode for quick dictation:

1. **Open any text field** (editor, browser, terminal, etc.)
2. **Press SUPER+Shift+D** to start recording
   - Waybar shows: `â— dictation` (red, golden background)
   - Hear: snare sound (drum hit)
3. **Speak clearly**: "This is a test of the whisper dictation system"
4. **Press SUPER+Shift+D** again to stop
   - Waybar shows: `dictation` (processing)
   - Hear: hihat sound
5. **Wait ~1-2 seconds** â†’ Text appears where your cursor is!

### 3. Server Mode (â—†)

Keeps the whisper model loaded in memory for faster transcription:

- **Enable**: Right-click waybar â†’ "Toggle to Server mode"
- **Benefits**: Faster transcription (no model loading time), better for larger models
- **Trade-off**: Uses more RAM (~577 MB for small.en, ~1.7 GB for large-v3-turbo)
- **Usage**: Same as CLI mode (SUPER+Shift+D), but faster

**Streaming Mode Details:**
- Uses `base.en` model (optimized for speed)
- 30-second audio buffer with VAD
- Types complete sentences after pauses
- No need to press keys while speaking
- Great for long-form dictation

### Waybar Indicator

The indicator shows the current mode and state:

| Icon | Mode | State | Meaning |
|------|------|-------|---------|
| ã€° | CLI | Ready | Ready to record (SUPER+Shift+D to start) |
| â— dictation | CLI | Recording | Recording your voice |
| dictation | CLI | Processing | Transcribing audio |
| â—† | Server | Ready | Model in memory, ready to record |
| â—† dictation | Server | Recording | Recording (faster transcription) |
| ã€° streaming | Stream | Active | Live streaming mode active (SUPER+D to stop) |

### Controls

- **SUPER+D** - Toggle live streaming mode (â–¶)
- **SUPER+Shift+D** - Toggle recording (CLI â— or Server â—† mode)
- **Left-click waybar** - Same as SUPER+D (toggle streaming)
- **Right-click waybar** - Open model/mode menu
- **Hover waybar** - See current model, mode, and controls

### Which Mode Should I Use?

| Use Case | Recommended Mode | Why |
|----------|------------------|-----|
| Quick notes, commands | **CLI Mode (â—)** | Low memory, good enough accuracy |
| Frequent dictation sessions | **Server Mode (â—†)** | Faster, no model loading delay |
| Long-form writing, transcription | **Stream Mode (â–¶)** | Hands-free, natural flow |
| Professional documents | **Server Mode (â—†)** with `small.en` or `medium.en` | Better accuracy, still fast |
| Maximum accuracy | **Server Mode (â—†)** with `large-v3-turbo` | Best quality (~14s per transcription) |

**Memory Usage Comparison:**
- CLI mode: ~300 MB (only during transcription)
- Server mode with base.en: ~577 MB (persistent)
- Server mode with small.en: ~577 MB (persistent)  
- Server mode with large-v3-turbo: ~1.7 GB (persistent)
- Stream mode: ~300 MB (uses base.en, only while streaming)

## Configuration

### Changing the Keybinding

Edit `~/.config/sway/config`:
```bash
# Change SUPER+D to something else (e.g., SUPER+M)
bindsym $mod+m exec ~/projects/asahi-whisper-daemon/toggle_dictation.sh
```

Then reload Sway:
```bash
swaymsg reload
```

### Using a Different Model

The installer uses `base.en` by default. You can choose a different model during installation:

```bash
./install.sh --model small.en    # More accurate
./install.sh --model medium.en   # Even better accuracy
./install.sh --model large-v3    # Best accuracy, slower
```

#### Available Models

**Main English Models (Recommended):**

| Model | Size | Speed (M2) | Accuracy | Best For |
|-------|------|------------|----------|----------|
| `tiny.en` | 75 MB | ~30x realtime | Basic | Quick notes, commands |
| `base.en` | 147 MB | ~11.5x realtime | Good | **Default** - balanced |
| `small.en` | 488 MB | ~4x realtime | Better | Longer dictation |
| `medium.en` | 1.5 GB | ~2x realtime | Great | Professional work |
| `large-v3` | 3.1 GB | ~1x realtime | Best | Maximum accuracy |
| `large-v3-turbo` | 1.6 GB | ~1.5x realtime | Excellent | Faster large model |

**Notes:**
- **English-only models** (`.en`) are faster and more accurate for English than multilingual versions
- **Quantized models** (`-q5_0`, `-q5_1`, `-q8_0`) are smaller with minimal accuracy loss
- **Multilingual models** (without `.en`) support 99 languages but are slower
- **Full model list**: Run `cd ~/projects/whisper.cpp/models && ./download-ggml-model.sh` to see all available models

#### Switching Models (Interactive)

**Right-click the waybar indicator** to open the model switcher:

![Model Switcher Menu]
```
â— base.en (active)      â† Currently using
âœ“ tiny.en              â† Downloaded
  small.en             â† Available to download
  medium.en            â† Available to download
  large-v3-turbo       â† Available to download
---
ðŸ“¥ Download more models...
```

- **â— symbol** = Currently active model
- **âœ“ symbol** = Downloaded and ready to use
- **No symbol** = Available to download (will download automatically when selected)
- **Download more models...** = Opens terminal to manually download models

The daemon will automatically restart with the new model when you select one.

#### Changing Model Manually

If you prefer command-line:

```bash
cd ~/projects/whisper.cpp
./models/download-ggml-model.sh small.en

# Edit the service file
nano ~/.config/systemd/user/whisper.service

# Change the --model parameter to:
# --model /home/YOUR_USERNAME/projects/whisper.cpp/models/ggml-small.en.bin

# Reload and restart
systemctl --user daemon-reload
systemctl --user restart whisper.service
```

### Streaming Mode Requirements

Streaming mode requires SDL2 to be installed and whisper.cpp to be built with SDL2 support:

```bash
# Install SDL2
sudo dnf install SDL2 SDL2-devel

# Rebuild whisper.cpp with SDL2 support
cd ~/projects/whisper.cpp
rm -rf build
cmake -B build -DWHISPER_SDL2=ON
cmake --build build --config Release
```

After rebuilding, `whisper-stream` will be available and streaming mode will work.

### Tuning Streaming Mode

You can adjust VAD sensitivity and buffer behavior by editing `toggle_stream.sh`:

**VAD Threshold (`-vth`):**
```bash
# More sensitive (triggers on shorter pauses)
-vth 0.7

# Default (waits for natural pauses)
-vth 0.6

# Less sensitive (good for continuous speech with background noise)
-vth 0.5
```

**Buffer Settings:**
```bash
# Audio buffer length (milliseconds)
--length 30000   # Default: 30 seconds

# Buffer overlap for continuity (milliseconds)
--keep 200       # Default: 200ms overlap

# Processing threads (adjust for your CPU)
-t 8             # Default: 8 threads (good for M1/M2/M3/M4)
```

**Advanced: Deduplication Algorithm**

The streaming mode uses a smart deduplication algorithm to prevent repeated text:

1. **Simple prefix matching** - Fast path for continuous speech
2. **Fuzzy word-based matching** - Handles buffer shifts and punctuation changes
3. **Automatic context reset** - Clears after 10+ seconds of silence to prevent slowdown

This ensures clean output even during:
- Buffer shifts (when audio window moves forward)
- Punctuation corrections ("Excellent." â†’ "Excellent,")
- Apostrophe handling ("it's", "I'm", "you're")
- Long pauses between thoughts

### Customizing Waybar Indicator

**Change the text/icons:**

Edit `waybar_whisper.py`:
```python
# CLI mode icon
icon = "â—"

# Server mode icon  
icon = "â—†"

# Stream mode icon
icon = "â–¶"
```

**Change colors:**

Edit `config/waybar-style.css` and apply to your `~/.config/waybar/style.css`.

### Disabling Audio Feedback

Remove or rename the sound files:
```bash
mv ~/projects/asahi-whisper-daemon/sounds ~/projects/asahi-whisper-daemon/sounds.bak
systemctl --user restart whisper.service
```

## How It Works

### CLI/Server Mode Flow

```
User presses SUPER+Shift+D
    â†“
Sway keybinding â†’ toggle_dictation.sh
    â†“
Unix socket â†’ whisper_daemon.py
    â†“
Audio recording (sounddevice) â†’ 16kHz mono WAV
    â†“
whisper-cli (C++) â†’ whisper.cpp model
    â†“
Transcribed text â†’ wtype (types it)
    â†“
Waybar indicator updates
```

### Streaming Mode Flow

```
User presses SUPER+D (start)
    â†“
toggle_stream.sh starts whisper-stream
    â†“
Continuous audio capture with SDL2
    â†“
Voice Activity Detection (VAD) detects speech/silence
    â†“
On silence: transcribe last N seconds
    â†“
Parse output â†’ Smart deduplication algorithm
    â†“
wtype types only NEW text
    â†“
Loop (until SUPER+D to stop)
```

### Architecture Components

**Core Services:**
- **whisper_daemon.py** - Main daemon (CLI/Server modes)
  - Runs as systemd user service
  - Handles audio recording via sounddevice
  - Manages whisper-cli subprocess
  - Server mode keeps model in memory
  
- **toggle_stream.sh** - Streaming mode controller
  - Spawns whisper-stream subprocess
  - Implements smart deduplication algorithm
  - Handles VAD output parsing
  - Auto-resets context on long silence

**IPC & State:**
- **Unix socket** - `/tmp/whisper_daemon.sock` (CLI/Server communication)
- **Flag files** - `/tmp/whisper_recording`, `/tmp/whisper_streaming`
- **Log files** - `/tmp/whisper_daemon.log`, `/tmp/whisper_stream.log`
- **Debug logs** - `/tmp/whisper_stream_debug.log` (streaming deduplication)

**Audio Pipeline:**
- **Input**: Default microphone (PipeWire/ALSA)
- **Format**: 16kHz mono WAV
- **Capture**: sounddevice (Python) for CLI/Server, SDL2 for streaming
- **Processing**: whisper.cpp with ARM optimizations

**Output:**
- **Text injection**: wtype (Wayland native)
- **Works in**: Any text field (terminal, browser, editor, etc.)
- **No clipboard pollution**: Direct keyboard input simulation

### Smart Deduplication Algorithm (Streaming Mode)

The streaming mode includes a sophisticated deduplication system to handle whisper.cpp's rolling buffer:

**Problem**: whisper-stream uses a 30-second sliding window. Each transcription includes previous content plus new speech, causing repetition.

**Solution**: Three-tier matching system:

1. **Simple Prefix Match (Fast Path)**
   ```
   Previous: "Hello world"
   Current:  "Hello world this is new"
   Result:   Types only "this is new" âœ“
   ```

2. **Fuzzy Word-Based Match (Buffer Shifts)**
   ```
   Previous: "Let's test if it's working"
   Current:  "working now with more text"
   Algorithm: Find longest word sequence overlap
   Result:   Types only "now with more text" âœ“
   ```

3. **Punctuation Normalization**
   ```
   Previous: "Excellent. This is cool."
   Current:  "Excellent, this is cool. More stuff."
   Algorithm: Compare words ignoring trailing punctuation
   Result:   Types only "More stuff." âœ“
   ```

**Automatic Context Reset:**
- Monitors timestamps from whisper output
- Detects silence gaps > 10 seconds
- Clears previous context to prevent slowdown
- Prevents memory buildup in long sessions

**Edge Cases Handled:**
- Apostrophes in contractions (it's, I'm, you're)
- Whisper's punctuation corrections
- Buffer shifts during long dictation
- Multi-word repetitions
- Partial word fragments

## Performance

### Resource Usage

**Always-On Streaming Mode** (base.en model on Apple M2):
- **Memory**: ~300 MB (persistent while streaming is active)
- **CPU**: 45% of one core during active transcription, <1% during silence
- **Battery Impact**: Minimal - only processes audio during speech (VAD)
- **Safe to leave on**: Yes! Automatically handles long sessions with silence detection

**CLI/Server Mode:**
- **Memory**: ~300 MB during transcription, 0 MB when idle (CLI), or ~577 MB persistent (Server)
- **CPU**: Only active during recording/transcription
- **Latency**: 1-2 seconds for 5-15 second clips

### Benchmarks (Apple M2, base.en model)

- **Speed**: 11.5x faster than real-time
- **Transcription time**: ~1-2 seconds for typical 5-15 second recordings
- **Model load time**: ~50ms (negligible in streaming/server mode)
- **CPU optimizations**: ARM NEON, FP16, DOTPROD acceleration

### Model Performance Comparison

| Model | Size | Speed | Accuracy | Memory (Stream) | Memory (Server) |
|-------|------|-------|----------|-----------------|-----------------|
| tiny.en | 75 MB | 30x realtime | Basic | ~100 MB | ~200 MB |
| base.en | 147 MB | 11.5x realtime | Good | ~300 MB | ~577 MB |
| small.en | 466 MB | 4x realtime | Better | ~500 MB | ~577 MB |
| large-v3-turbo | 1.6 GB | 1.5x realtime | Excellent | ~1.7 GB | ~1.7 GB |
| large-v3 | 2.9 GB | 1x realtime | Best | ~3 GB | ~3 GB |

### Long Session Support

**Streaming mode is designed for extended use:**
- âœ… **Automatic context reset** after 10+ seconds of silence
- âœ… **Intelligent deduplication** prevents repeated text
- âœ… **Fuzzy punctuation matching** handles whisper's self-corrections
- âœ… **Buffer management** handles hours of continuous dictation
- âœ… **No memory leaks** - tested for multi-hour sessions

**Can I leave streaming on all the time?**
Yes! The streaming mode only uses CPU/resources when you're actually speaking. During silence:
- VAD (Voice Activity Detection) waits for speech
- CPU usage drops to <1%
- Memory stays constant at ~300 MB
- No performance impact on other applications

## Management

### Service Commands

```bash
# Check status
systemctl --user status whisper.service

# Restart daemon
systemctl --user restart whisper.service

# Stop daemon
systemctl --user stop whisper.service

# View logs
tail -f /tmp/whisper_daemon.log

# Or systemd logs
journalctl --user -u whisper.service -f
```

### Testing Connection

```bash
# Check if daemon is responsive
echo "STATUS" | ncat -U /tmp/whisper_daemon.sock

# Should return: READY or RECORDING
```

## Troubleshooting

### Text doesn't appear

**Problem**: Recording works but no text is typed.

**Solutions:**
1. Ensure cursor is in a text field
2. Check wtype is installed: `which wtype`
3. Check logs: `tail -f /tmp/whisper_daemon.log`
4. Look for transcription output in logs

### Waybar indicator shows â— (error state)

**Problem**: Daemon isn't running.

**Solutions:**
1. Start the service: `systemctl --user start whisper.service`
2. Check status: `systemctl --user status whisper.service`
3. View errors: `journalctl --user -u whisper.service -n 50`

### SUPER+D doesn't work

**Problem**: Keybinding not responding.

**Solutions:**
1. Reload Sway: `swaymsg reload`
2. Check keybinding exists: `grep whisper ~/.config/sway/config`
3. Try running toggle script manually: `~/projects/asahi-whisper-daemon/toggle_dictation.sh`

### No sound feedback

**Problem**: No audio plays on start/stop.

**Solutions:**
1. Check sound files exist: `ls ~/projects/asahi-whisper-daemon/sounds/`
2. Test audio: `paplay ~/projects/asahi-whisper-daemon/sounds/snare.wav`
3. Check system volume isn't muted

### Slow transcription

**Problem**: Takes too long to process.

**Solutions:**
1. This is normal for first transcription (~2s as model loads)
2. Subsequent transcriptions should be ~1s
3. Using large models will be slower
4. Keep recordings under 15 seconds for best speed

### Streaming mode not deduplicating properly

**Problem**: Text is being repeated or cut off in streaming mode.

**Solutions:**
1. Check debug log: `tail -f /tmp/whisper_stream_debug.log`
2. Look for `[DEBUG] Found fuzzy overlap` - this means deduplication is working
3. Look for `[DEBUG] No overlap found, typing everything` - might indicate buffer shift issues
4. Restart streaming mode: Press SUPER+D twice (off/on)
5. If persistent, check for apostrophe handling in log

**Debug example (working correctly):**
```
[DEBUG] Captured timestamp line: '[00:00:00.000 --> 00:00:05.000]   Hello world'
[DEBUG] Extracted text: 'Hello world'
[DEBUG] Found fuzzy overlap at word 0 (2 words matched) -> new_text: 'this is new'
[DEBUG] Typing: 'this is new '
[DEBUG] wtype succeeded
```

### Advanced debugging

**Enable verbose logging for streaming:**
```bash
# Watch all logs simultaneously
tail -f /tmp/whisper_stream.log /tmp/whisper_stream_debug.log /tmp/whisper_stream_output.log

# Or in separate terminals:
tail -f /tmp/whisper_stream.log           # whisper-stream stderr (model loading, VAD events)
tail -f /tmp/whisper_stream_output.log    # Raw whisper output (transcription blocks)
tail -f /tmp/whisper_stream_debug.log     # Our deduplication algorithm debug
```

**Inspect the deduplication algorithm:**
```bash
# See what text is being captured and matched
grep "Extracted text" /tmp/whisper_stream_debug.log

# See deduplication decisions
grep "overlap" /tmp/whisper_stream_debug.log

# See what's being typed
grep "Typing:" /tmp/whisper_stream_debug.log
```

For more detailed troubleshooting, see [TROUBLESHOOTING.md](TROUBLESHOOTING.md).

## Frequently Asked Questions

### Can I leave streaming mode on all the time?

**Yes!** Streaming mode is designed for always-on use:
- Only uses CPU when you're actively speaking
- VAD waits silently when you're not talking
- Memory stays constant at ~300 MB
- Automatically resets context after 10s silence
- No performance impact on other applications
- Battery-friendly (minimal CPU during silence)

### What's the difference between the three modes?

| Aspect | Stream Mode | CLI Mode | Server Mode |
|--------|-------------|----------|-------------|
| **Startup** | Always ready | 50ms model load | Always ready |
| **Memory** | 300 MB while active | 0 MB when idle | 577 MB persistent |
| **Best for** | Long dictation | Quick notes | Frequent short recordings |
| **Deduplication** | Smart algorithm | Not needed | Not needed |
| **Activation** | SUPER+D (toggle) | SUPER+Shift+D (hold) | SUPER+Shift+D (hold) |

### How long can I speak in streaming mode?

**Indefinitely!** The system handles:
- âœ… Hours of continuous dictation
- âœ… Automatic buffer management (30s rolling window)
- âœ… Smart deduplication prevents repetition
- âœ… Auto-reset after long pauses (>10s)
- âœ… No memory leaks or slowdown

The 30-second buffer is a rolling window - older audio gets dropped automatically while new speech is captured.

### Why do I sometimes see duplicated words?

This can happen when:
1. **Whisper corrects itself** - Changes "Excellent." to "Excellent," between transcriptions
2. **You pause mid-sentence** - VAD might transcribe partial thoughts
3. **Buffer shifts** - Should be rare with our fuzzy matching algorithm

If you see consistent duplication, check `/tmp/whisper_stream_debug.log` and report an issue.

### What happens if I pause for a long time?

After **10+ seconds of silence**:
- Context automatically resets
- Next transcription starts fresh
- Prevents slowdown from comparing huge texts
- This is intentional and expected behavior

You'll see in debug log: `[DEBUG] Long silence detected (XXXXms), resetting context`

### Can I use this for transcribing meetings?

**Yes, but with caveats:**
- Works best for **your own speech** (single speaker)
- Struggles with **multiple speakers** (no speaker diarization)
- Use **Server mode with large-v3-turbo** for best accuracy
- Consider a good quality microphone
- Or use CLI mode and record segments manually

### Does it work offline?

**100% offline!** Everything runs locally:
- No internet required
- No cloud services
- Complete privacy
- Works on airplanes, remote areas, etc.

### What about punctuation?

Whisper adds punctuation automatically:
- Periods, commas, question marks
- Capitalization
- Some basic formatting
- Works best with clear, natural speech

## Tips for Best Results

**General:**
- **Speak clearly** with good enunciation
- **Minimize background noise** for better accuracy  
- **Speak at normal pace** - not too fast or slow
- **Use in quiet environments** when possible

**Streaming Mode Specific:**
- **Pause naturally** between sentences (helps VAD)
- **Wait for text** before continuing (gives system time to process)
- **Speak in phrases** rather than very long run-on sentences
- **Use long pauses** (10+s) between different topics to reset context

**CLI/Server Mode Specific:**
- **Keep recordings short** (5-15 seconds ideal)
- **Pause before/after** speaking to ensure clean audio
- **Wait for beep** before speaking (recording started)

## Contributing

Contributions welcome! Please feel free to submit issues or pull requests.

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Credits

- **whisper.cpp**: https://github.com/ggerganov/whisper.cpp
- **OpenAI Whisper**: https://github.com/openai/whisper
- **Asahi Linux**: https://asahilinux.org/

Built with inspiration from the desktop whisper-dictation-daemon setup.

## Acknowledgments

Special thanks to:
- The Asahi Linux team for making Linux on Apple Silicon possible
- The ggerganov team for whisper.cpp
- OpenAI for the Whisper model
